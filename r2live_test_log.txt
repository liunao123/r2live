r2live发现的问题



1、在initialStructure过程中，首先用到的structure from motion（SFM），
   利用相邻帧上的特征点，来恢复出相机运动。
   只要特征点不是很少， SFM过程基本可以顺利完成。
   


2、图像恢复出的尺度因子经常是 负 的，
   在 1 描述的过程中，如果 SFM 成功，则会进行visualInitialAlign过程；
   此时，先通过VisualIMUAlignment来计算 IMU 的gyroscope bias，然后进行LinearAlignment过程，
   把相机的所有位置，重力加速度g，单目图像的尺度s；放到一起进行求解。
   一旦所估计出的重力加速度和IMU测量的差距过大，或者尺度因子为 负值 。此时 visualInitialAlign 过程失败，。
   就会出现 “Misalign of visual structure with IMU”的错误。
   
   目前分析来看，到此，起码图像的处理已经完成，但是 与 IMU 的测量有冲突，导致失败。


3、指定的外参旋转矩阵会被转成四元数，然后归一化。
   不同的旋转矩阵会对应一个位姿。
   当使用VINS在线估计出来的外参（绕3个轴的旋转角度）（176.517  -175.92  -179.874）比
                                    单独的标定得到的（171.864  -93.7768 -82.1446） 旋转看起来要符合实际情况。
   但是很明显，二者的方向是不一样的。
   测试发现：
   且 单独标定的外参 所指定的相机方向 和实际方向不一致，但 VINS在线估计出的相机方向 是 符合预期的。
   
   如果在进行建图时，采用固定不变的外参，则程序会有 “ 旋转矩阵不正交的 错误”， 然后挂掉。


4、最容易出现的错误：
Misalign of visual structure with IMU 《VisualIMUAlignment对齐有问题：estimator.cpp 408 有问题》
big IMU acc bias estimation
 github 上 作者的回复：
 Hi, firstly I suggest you to check your IMU stream, e.g. whether the IMU reading has a large bias.
 You can do integration and double-integration on IMU readings, the trajectory should be normal for seconds.
 把相邻的两个IMU数据取均值，这样频率降为原来的 一半，效果看起来区别不大。
 big IMU acc bias estimation 则是初始化的时候，得到的


5、目前来看，改变imu的噪声水平，对实时位姿的估计影响较大。
   目前利用imu_ultiz 估计出来的IMU噪声，如果将其扩大数倍（*2， *3 ， *5）
   扩大 ，可以减小reboot的频率, 但是点云的偏移部分会被放大，位姿会不稳定，重影会出现。
   当降低IMU的噪声水平时，地图的偏移会减小很多。


6、在建图过程中估计出的状态量（位姿，速度，零偏，重力加速度），会被进行 check ，
   测试发现 全局变量 g_lio_state 会出现速度过大的情况，
   
   在某一时刻的状态，其速度大于 10 时，系统旋转的估计有问题，给出提示，此时建图过程会跑偏。
   
   有2个位置会对其进行check， 并且大部分 时候的错误（估计出的速度过大），
   
   都是在 IMU_Processing.cpp 的imu_preintegration 函数中。
   当偏移发生时，并且这个速度 会 逐步增大很多。



7、硬件的硬同步问题，数据源之间的时间同步很重要，
目前来看，测试更改了图像的时间戳，缓存数量使得最新的图像可以及时发出去，明显减小图像的延迟。
但是图像与IMU的实际延迟依然不可得，目前让其在线估计该时间延迟，量级在0.001s级别。

   参数，可以调整一下
   #unsynchronization parameters
   estimate_td: 1                      # online estimate time offset between camera and imu
   td: 0.003--0.007                           # initial value of time offset. unit: s. readed image clock + td = real image clock (IMU clock)。


8、VIO部分单独测试，也是可以运行的。并且回环也可以检测到
   运行的稳定性和数据有关，
   在r2live容易报错的地方，VIO原来的代码是没有的。如：IMU_Processing.cpp； estimator_optimization.cpp;
   
   
9、建图的分辨率，
0.1m 很容易漂移
0.4m 默认的参数


参数保持与demo一致。

按以下思路进行测试：
1、IMU数据单独拿出来，看一下是否正常（用另一个IMU作为参考）；
2、重力加速度的量级，是否正常（正常端着走）；
3、IMU数据要对应到视觉上；


文件夹里的r2live-master测试版本：


20220810
1、目前的参数，楼下大圈和小圈的数据，vins都可以运行，但是尺度估计的不对，绕楼下几圈，轨迹会越来越小。
2，官方训练的数据集，今天测试下来对楼下室外环境基本检测不到回环。但是回环检测的节点却可以输出位姿，更新频率数秒一次，很低。并且与状态估计节点之间的topic里一直没有数据交互。

20220811
1、VINS的回环检测正常，将pose_graph的回环检测已经加入到R2live中，并且二者的数据交互正常；
2、位姿目前仅仅pose_graph可以通过检测到的回环进行纠正，r2live的轨迹还是偏移，然后点云地图更是如此；
3、回环检测使用的词袋文件（brief_k10L6.bin）是通用的。
4、启动pose_graph所需要的4个参数，目前使用：0 0 2 0.1，感觉良好。


20220812
1、estimator_node.cpp 第741行的 状态更新暂时注释掉了
2、


20220816
1、回环现在把优化的位姿保存下来，然后通过另外的程序结合 点云 的时间戳 进行插值
2、目前对 高度方向进行的 插值优化，感觉良好，但是水平方向上的偏移依然无法得到解决。

3、把fast_lio里面的位姿和点云也保存下来，然后只优化这个位姿的 Z 值，实现上述1和2。
4、common_lib.h里面关于 Lidar_offset_to_IMU 变量，需要注意。而且代码里面与官网提供的不一样。

20220829
测试发现 image 与 IMU的时间差，影响较大。应该先通过在线估计的形式，得到一个较好的 初始值。
然后将该初始值 再让其 在线估计，直到该数据稳定。

#unsynchronization parameters
estimate_td: 1                      # online estimate time offset between camera and imu
td: -0.02                             # initial value of time offset. unit: s. readed image clock + td = real image clock (IMU clock)

2022年8月手持平台测试情况：
1、目前以步行的速度录制的数据，轨迹长度1800m，持续25分钟，建图可以稳定进行。回到起点位置附近，LIO部分输出的原始点云高度方向偏差约0.5m。水平偏移约1.6m。建图的稳定性、精度比之前都有改善。
2、建图参数 image 与 IMU的时间差，影响较大。目前先通过在线估计的形式，得到一个较好的 初始值（目前我们的是：-0.01s），然后以该值再进行建图，系统稳定性有所提高。
3、视觉回环部分可以正常工作，但是其回环结果没有与建图程序联系起来，无法直接利用回环结果去优化点云地图。测试利用插值的方式去单独优化每帧点云对应的位姿，得到的点云地图依然存在水平方向上的偏移，且位姿的方向不对，导致点云的细节很模糊。
4、基于GTSAM的回环检测目前正常进行，得到的结果较之前有所改善，但是由于回环目前采用关键帧逐一遍历的方式进行，所以搜索效率较低，很耗时。采用将建图开始时和结束的一段数据，跳过中间的数据，进行优化。
   当检测到回环后，GTSAM利用GICP得到的位姿变换，可以将点云在高度、水平方向的偏移纠正回来，虽然点云地图远处的细节有点模糊，但近处的细节保留的较好。
5、为了加快回环的速度，目前将以视觉检测到的回环为准，保留该回环发生的时间，然后点云的回环将以该时间为准，不再进行暴力遍历的方式进行。具体检测的效率如何仍在测试。
6、计划再采集更长的数据，测试建图效果

2022年9月5-9号工作汇总：
1、测试对于livox点云的特征点提取代码，目前利用loam_livox里面的代码可以将平面点提取出来。而r2live里面点和面都可以提取出来，但是后续的r2live后续只用到了平面点。
2、本周又录了2个包，一个是绕园区外侧一周，可以稳定建图，利用GTSAM优化后的点云效果良好。另一个绕园区主干道，用时40分钟，长度3000m左右。建图稳定，而GTSAM的优化效果尚不理想，仍在优化。
   目前2个思路，其一：调整回环检测的条件，增加检测到的回环数，其二：利用上述提取的平面点，来尝试提高GICP的匹配精度。

2022年9月19-23号工作汇总：
1、测试不同参数对点云地图优化的影响，现在发现0.5m的关键帧阈值， 0.36的GICP阈值较为合适，过小的阈值会使得部分细节变得模糊，并且导致回环过少。
2、点云地图在轨迹交叉，但是没有回环的位置容易发生重影，其中部分位置通过手动加入回环可以将重影消除，其余轨迹交叉的位置是很难有回环的。
3、在部分轨迹重复的区域，依然不能如预期那样检测到回环，目前综合考虑，怀疑是某条轨迹上存在动态障碍物，这样点云的特征不再一致，导致匹配出问题，进而检测不到回环。
4、机械的同事将新的相机安装上，利用视觉组的电子标定板和程序，内参标定完成。特征提取偶尔还是会有蓝线出现，这是对特征点提取错误，后续测试对建图影响不大。
5、到楼下采集一点数据进行在线外参标定，在线外参标定基本正常，利用标定后的外参近建图也可以正常进行。

2022年9月26-30号工作汇总：
1、更换新的相机后，进行内外参的标定。其中外参以在线的方式完成。目前标定完成，可以正常使用。
2、测试重复轨迹较多的情况下，GTSAM的优化情况。在每栋楼大概绕了1.5圈的方式进行建图，这样每栋楼都会有0.5圈的长度上有机会检测到回环。
3、在重复轨迹较多的数据上，发现：较密集（0.3--0.5m）的关键帧并不能得到理想的效果，而稍微稀疏（0.75m）一点的关键帧效果会更好一些。但是此时的点云数量会下降，不知对后续定位的影响有多大。
4、以上述方式所得到的地图，目前在效果比之前有所改善，目前测试了20分钟约2000m的建图轨迹，优化后的地图基本没有重影。
5、开始基于上述的地图，测试ndt_localizer的定位效果，以建图的bag来模拟定位过程，发现定位不稳定，每次给定初始位置后，定位也会在不久后丢失。
6、针对目前定位的问题，后续一方面学习调参，另一方面调研点云+视觉的定位方法。

2022年10月8-14号工作汇总：
1、测试路线：每栋楼绕了大概1.5圈的数据集，此时检测回环的数量会增加很多，此时每栋楼局部的约束很多，在开始和结束的位置上再检测到一些回环，基本可以得到效果不错的地图。
2、在上述路线中发现：某些相似的环境中，GICP给出的回环有可能是错误的，此时匹配得分很低，但是是约束不对，导致优化出现问题，这是相似环境导致的。
3、测试先大圈后小圈的路线：发现在重复路线较多的位置上，回环检测没有问题，该位置的地图也没有问题，但是重复路线较少（如只重复了一栋楼的宽度）的位置依然无法稳定的检测出回环，也就无法优化该位置的地图。
4、调研测试适用于livox的激光定位算法：之前的lidar_localizatuon之类的算法都是针对机械激光雷达设计的。livox_localization和fast_lio_localization是适用livox的。初步测试发现：
   前者需要指定一个较为精确的初始值，测试发现在rviz里指定一个大致的位置，程序很难重定位成功，后者则需要生成关键帧的scan context在定位时使用，目前在尝试生成我们自己的scan context。
5、测试Livox-Mapping来生成点云对应的scan context，以及利用scancontext的开源代码测试生成定位用的文件。


2022年10月17-21号工作汇总：
手持建图工程化 60%
1、地图优化与建图整合到一起，现在二者可以同时进行，不过目前运行的很慢，
   主要是后期内存(pose_graph)占用很高。：// TODO 内存泄漏？
   小一点的数据上测试，没有问题。较长的路线正在测试......
   service call /end_gtsam {}

手持定位 20%
视觉激光融合：
2、2D-3D-pose-tracking（https://github.com/Zumbalamambo/2D-3D-pose-tracking）依赖CUDA，
   在测试没有GPU的情况下咋样。
3、3D line detection部分代码运行成功；C:\Users\xf\Desktop\3d.png
纯激光：
4、livox_localization之前初始化较慢，甚至长时间不能重定位到正确的位置上，
   猜测是目前利用keyframe生成的scan context密度小。(0.75m)。计划使用较密的scan context测试。

定位：纯激光 + 短距离的里程计(长走廊环境)
列检机器人，坑道环境（办公室走廊-定位功能做完之后）
问题的复现：

2022年10月24-28号工作汇总：
1、利用valgrind工具排查，pose_graph有无内存泄露出现.不过很小，几kb的量级。
   在测试其关键帧new出来后，手动delete，发现运行期间程序会挂掉。
2、先大圈再小圈的建图路线，测试发现，在某些重复轨迹上，视觉也没有检测到回环，就很奇怪。
3、livox_localization更改一些参数，以及加大scan contextd密度后（0.3m/个），可以进行重定位。
   3.1 开始阶段重定位一旦收敛到准确的位置上，后面可以一直定位下去。建图和定位用同一段bag文件。发现定位结果在Z轴方向上存在一定的偏差。
   3.2 当建图与定位不是同一个bag文件时，地图的动态变化部分，会使得定位有较大波动。
   3.3 目前发现在某些位置上（视野开阔的地方），在5s左右可以重定位完成，而在某些位置上，会一直无法重定位成功。
5、尝试给livox_localization添加一个手动重定位的功能，现在其可以收到在rviz里给出的位置，但重定位结果有问题，仍需测试。
4、2D-3D-pose-tracking 里用到的 AFM ，需要cuda的库，暂时无法进行测试。
   发现了一个非ros的视觉点云定位算法：https://github.com/hyye/dsl，也需要cuda。

视觉组的电脑
2022年11月01-11号，手持导航平台工作汇总：
1、定位的初始位姿，通过手动运行程序来指定。
   初始位姿是通过：livox_localization中的scan context来获取的。然后通过“/initialpose”发布出去。完成

2、基于点云地图的定位，尝试以r2live的视觉激光融合里程计来输出。
   基于1：手动指定一个精确的初始位姿，将r2live前端里程计的起点转换到初始位姿上。从上述位姿开始建图，同时也得到了基于地图的位姿。完成
   
   以上述方式得到的，存在2个问题：1：过分依赖初始值；2：r2live的原始里程计的精度是什么样就什么样，无法得到改善。

3、ssl_slam2（intel的固态激光雷达）的角点+ 面点 正在测试。
   目前从地图中分离 角点 和 面点 有问题，感觉提取之后就不再是点云了。

后期工作：
1、下周调研确定：全局定位算法（scan context or icp or 粒子滤波 ...... ）确定用什么
2、r2live前端里程计 测试稳定下来，短时间内的里程计要稳定。
//TODO: 二者结合


2022年11月14-18号
1、点云相似度的库：https://github.com/Dongxu05/CSSC
   在没有指定初始值的情况下比GICP快很多

需要GPU
2、基于分割的定位算法：https://github.com/ethz-asl/segmap
3、深度学习定位算法： 
             https://github.com/PRBonn/overlap_localization
             https://github.com/PRBonn/range-mcl
             https://github.com/JuanDuGit/DH3D

纯激光的定位定位算法：
4、全局重定位的算法 : https://github.com/koide3/hdl_global_localization
                    https://github.com/koide3/hdl_localization
   在机械雷达下，可以实现全局定位功能，有图
   livox雷达，全局定位不行，给定初始位姿，收敛的很快，计算速度也可以跟上
   
   
5、https://github.com/carlosmccosta/dynamic_robot_localization
   利用 livox 激光数据进行测试，发现：
   1：tf tree是完整的，但是定位位姿没有输出
   2：所指定的初始位姿，没有起作用，会强制从原点可以定位。
   3：需要一个里程计数据，现在是通过r2live给出的

6、r2live的简化成前端里程计，现在单独形成一个节点，输出位姿，配合其他节点一起使用。
   1；r2live_locate 前端
   2: publish_initial_pose.py 发送初始值
   3：reprject_r2live_path 根据初始位姿，将r2live的位姿变换过去。

后续计划：
   1、实验室建图：原来的定位方式，对比现在的定位咋样，
   室外，合肥场地试一下。
      
   手持建图 + 机器人定位
   
   
   室外：手持建图 + 手持定位可以用
   
2022年11月21-25号
1、 https://github.com/koide3/hdl_global_localization
   全局定位算法，提供三种实现：
   1、BBS, 
   2、FPFH_RANSAC
   3、FPFH_TEASER
   第一个，官方的机械激光可以完成自动重定位工作，目前测试发现不适用于livox雷达点云。
   另外2个，简单测试，发现其接口无法在返回重定位的结果。
   
2、室内建图测试：
   1、走廊环境绕一圈，存在部分漂移。
   2、单纯的办公室环境，不用优化，也可以得到较好的地图。

3、室外定位测试：
   结合IMU数据，https://github.com/koide3/hdl_localization 可以完成大部分位置的定位工作。
   在用10月份的地图，结合8月份的数据，进行测试 发现：
   有以下位置会定位失败
   1、快速的旋转；
   2、在树比较多的位置；
   3、地图点云较少

4、室内定位测试：
   1、静止状态下，hdl的精度6cm左右。
   2、部分场景，单纯的激光，没有iMU会导致定位失效

   <!-- 全局定位先放一下 -->
   1、r2live前端里程计，融入hdl的odom predict里面去
   2、看一下hdl里面的tf在哪个地方发布的
  //TODO  3、室内外的 重复定位精度
   4、手持导航平台2.0的标定工作

2022年11月28号-12月2号
r2live
整体建图可以正常使用，无明显问题，简化版本的里程计也可以稳定输出
1、室内，将WINDOW_SIZE从7改为9/11/15，稳定性好很多。但是初始化global SFM的时间越长。
2、建图中的视觉失败，利用OpenCV里面的5点法，求解相邻帧的位姿很容易失败，
   导致错误。其实还是特征不够


hdl定位 
3、仅使用点云与imu, 大部分位置都是正常的， --- 在走廊拐弯的地方，点云会全部打在对面墙上，导致定位发生漂移。
4、融合r2live的里程计，用bag测试，
   4.1 里程计长时间运行后，数据处理跟不上，导致tf的时间戳有延迟。现在在测试修改数据的时间戳 
   4.2 r2live的里程计，其在高度方向上长时间的累计漂移仍存在，hdl给出的定位没有明显的高度问题。

5、机器人测试室内手持平台所建地图，办公室内可正常定位，测试精度在4cm以内

后续计划：
6、// 修改数据的时间戳，保证tf时间戳可用，用bag包测试。
7、融合r2live的里程计，进行实时定位测试，室内 + 室外。
8、手持平台2.0，标定工作。

  数据 丢 一些，保证实时性。

2022年12月5号-12月9号

手持平台配置： i7 8550U 1.8Ghz * 8

1、数据时间戳的问题，是 use_sim_time 要和 --clock 配合使用。、
2、定位实时性与2个方面有关：
   2.1 里程计的稳定性
   2.2 hdl的计算效率，其中初始化阶段较慢，如果指定了一个正确的初始位姿，定位会稳定很多。
3、为了保证里程计的稳定输出，对原始数据的缓存进行了限制，点云和视觉均保留最近 1s 内的数据。
4、在手持平台上，点云以 8hz 的输入，在 速度为 1m/s 左右时，单纯的里程计也可以稳定输出。一旦开启hdl，数据将会不能实时处理
5、手持平台，平面点（/laser_cloud_flat）作为 r2live 和 hdl 的共同输入，
   hdl无法获取 点云所在时间上的 里程计信息，二者的延迟在 0.4s 左右（里程计计算的时间+数据传输时间）
   此时，hdl的定位输出 在 3hz 左右。 -------- 1小时34分

后续计划：
6、手持平台2.0，标定工作。
7、在i7电脑上加内存条，再测试 融合里程计 后的hdl定位效果。
8、继续优化 在手持平台上 的 定位实时性。